# parallels
(time: секунды - везде в тексте + все эксперименты проводились на сервере НГУ)

Результаты экспериментов.

  Для double:
  
    1. Sum + initialization (GPU): 1.012548; time: 0.036696

    2. Sum + initialization (CPU): -0.000779; time: 0.070967

  Для float:
  
    1. Sum + initialization (GPU): 1.012548; time: 0.019097

    2. Sum + initialization (CPU): -644531.875000; time: 0.075783

Таким образом можно увидеть, что точность double на GPU примерно такая же, как и для float.
Если говорить о CPU, то здесь можно увидеть колосальное различие в точности: для double в принципе самая высокая точность среди всех испытаний, а вот для float мы видим, что точность как раз таки наоборот самой низкой оказалась.

Также видно, что на GPU операции с float происходят примерно в 2 раза быстрее, чем с double.

Вывод:

float и double на GPU имеют примерно одинаковую точность расчётов, хотя и уступают double на CPU. float CPU имеет самую низкую точность расчётов.

Время копирования с CPU на GPU:

    delta time 'pragma acc data copy(new_sin[0:N])': 3.197533 - достаточно долго

О том, как работает программа на GPU.

Для выполнения программы на GPU выделяются блоки ядер (32 на NVIDIA), все ядра выполняют те же инструкции, но с разными данными. Такой подход помогает быстрее проходить цикл и выполнять простые рутинные операции, вроде суммирования и инициализации данных.

